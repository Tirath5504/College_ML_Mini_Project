{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-O-KTxkynd6N"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-community gradio_client python-dotenv langsmith transformers torch tensorflow tf_keras scikit-learn openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jloG1RtZndBR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from gradio_client import Client, handle_file\n",
        "from langsmith import traceable\n",
        "from dotenv import load_dotenv\n",
        "from typing import Union, List, Dict, Any\n",
        "import json\n",
        "import mimetypes\n",
        "import logging\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2hc6r3injY3J"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mBjw-HPAjY3J"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('LLAMA_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUtatAo-jY3J",
        "outputId": "295ee6a5-526f-4043-b739-3076f62f0a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "classifier = pipeline(\"text-classification\", model=model_id, top_k=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pwE2QEwTqJYI"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"meta-llama/llama-4-maverick:free\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=API_KEY,\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OviEf0T2jY3J"
      },
      "outputs": [],
      "source": [
        "def create_client(client_id: str) -> Client:\n",
        "    return Client(client_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XN33j9YkjY3K"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def run_text_model(text: str, client_id: str) -> str:\n",
        "    try:\n",
        "        client = create_client(client_id)\n",
        "        label, score = client.predict(text, api_name=\"/predict\")\n",
        "        return f\"Prediction: {label} with a score of {score}\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in text model {client_id}: {e}\")\n",
        "        return f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "STo808GfjY3K"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def run_hinglish_hate_model(text: str) -> str:\n",
        "    return run_text_model(text, \"dj-dawgs-ipd/IPD-Hinglish-Text-Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GYVrAwJRjY3K"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def run_english_hate_model(text: str) -> str:\n",
        "    return run_text_model(text, \"dj-dawgs-ipd/IPD-Text-English-Finetune\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1SQjBuCvjY3K"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def run_image_hate_model(image_path: str) -> str:\n",
        "    try:\n",
        "        client = Client(\"dj-dawgs-ipd/IPD_IMAGE_PIPELINE\")\n",
        "        result = client.predict(handle_file(image_path), api_name=\"/predict\")\n",
        "        return f\"Image Analysis Result: {result}\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in image model: {e}\")\n",
        "        return f\"Error calling Image model: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sgXOVQ22jY3K"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def run_audio_hate_model(audio_path: str) -> str:\n",
        "    try:\n",
        "        client = Client(\"dj-dawgs-ipd/IPD-Audio-Pipeline\")\n",
        "        result = client.predict(handle_file(audio_path), api_name=\"/predict\")\n",
        "        return f\"Audio Analysis Result: {result}\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in audio model: {e}\")\n",
        "        return f\"Error calling Audio model: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uIAAS6dmjY3K"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def run_sentiment_analysis(text: str) -> str:\n",
        "    try:\n",
        "        prediction = classifier(text)\n",
        "        return prediction\n",
        "        # client = Client(\"https://ck42-sentiment-model-comparison.hf.space/--replicas/ftdum/\")\n",
        "        # sentiment_data = client.predict(text, api_name=\"/predict_2\")\n",
        "        # if isinstance(sentiment_data, list) and all(isinstance(item, (list, tuple)) for item in sentiment_data):\n",
        "        #     formatted_sentiments = \", \".join(\n",
        "        #         [f\"{sentiment} ({score:.2f})\" for sentiment, score in sentiment_data]\n",
        "        #     )\n",
        "        # else:\n",
        "        #     formatted_sentiments = str(sentiment_data)\n",
        "        # return f\"Sentiment Analysis: {formatted_sentiments}\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in sentiment analysis: {e}\")\n",
        "        return f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FRQvIl6MqNdF"
      },
      "outputs": [],
      "source": [
        "hinglish_tool = Tool.from_function(\n",
        "    func=run_hinglish_hate_model,\n",
        "    name=\"HinglishHateChecker\",\n",
        "    description=\"Detects hate speech in Hinglish text.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VLUI69gJjY3K"
      },
      "outputs": [],
      "source": [
        "english_tool = Tool.from_function(\n",
        "    func=run_english_hate_model,\n",
        "    name=\"EnglishHateChecker\",\n",
        "    description=\"Detects hate speech in English text.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dOgh5O6UjY3K"
      },
      "outputs": [],
      "source": [
        "image_tool = Tool.from_function(\n",
        "    func=run_image_hate_model,\n",
        "    name=\"ImageHateChecker\",\n",
        "    description=\"Detects hate symbols or hateful content in images.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tUbLAYFAjY3K"
      },
      "outputs": [],
      "source": [
        "audio_tool = Tool.from_function(\n",
        "    func=run_audio_hate_model,\n",
        "    name=\"AudioHateChecker\",\n",
        "    description=\"Detects hate speech in audio files.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BO4tA5TMjY3K"
      },
      "outputs": [],
      "source": [
        "sentiment_tool = Tool.from_function(\n",
        "    func=run_sentiment_analysis,\n",
        "    name=\"SentimentAnalyzer\",\n",
        "    description=\"Analyzes the sentiment of text to assess emotional tone.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6oSum6RFqWqI"
      },
      "outputs": [],
      "source": [
        "tools = [hinglish_tool, english_tool, image_tool, audio_tool, sentiment_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "brGpSceMjY3L"
      },
      "outputs": [],
      "source": [
        "def detect_modality(input_data: Union[str, os.PathLike]) -> str:\n",
        "    if isinstance(input_data, str):\n",
        "        if os.path.exists(input_data):\n",
        "            mime_type, _ = mimetypes.guess_type(input_data)\n",
        "            if mime_type:\n",
        "                if mime_type.startswith(\"image\"):\n",
        "                    return \"image\"\n",
        "                elif mime_type.startswith(\"audio\"):\n",
        "                    return \"audio\"\n",
        "            return \"unknown\"\n",
        "        else:\n",
        "            return \"text\"\n",
        "    return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cHXZKLYFqa5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b54ceac-9831-47d6-ca26-3df47c200ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-e0edd1dc2c28>:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ],
      "source": [
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tVZFgmBtjY3L"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def pipeline(input_data: Union[str, os.PathLike]):\n",
        "    modality = detect_modality(input_data)\n",
        "    system_instruction = f\"\"\"\n",
        "You are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\n",
        "\n",
        "Modality Detected: {modality}\n",
        "\n",
        "Use the following tools as applicable:\n",
        "- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\n",
        "- For image files: use the ImageHateChecker.\n",
        "- For audio files: use the AudioHateChecker.\n",
        "\n",
        "Instructions:\n",
        "1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\n",
        "2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING.\n",
        "4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\n",
        "   - \"Hate Speech: Yes\" or \"Hate Speech: No\"\n",
        "   - Followed by \"— Reason:\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\n",
        "\n",
        "Final Output Format:\n",
        "\"Hate Speech: Yes/No — Reason: <your detailed explanation with all tool outputs and reasoning>\"\n",
        "\n",
        "IMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING.\n",
        "SOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\n",
        "\n",
        "Now, process the following input:\n",
        "Input: {input_data}\n",
        "\"\"\"\n",
        "    prompt = f\"{system_instruction}\\n\\nInput: {input_data}\"\n",
        "    return agent.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7UjUuLMnayr",
        "outputId": "4c8efc97-d0e3-4818-d320-c12111b397a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The input is an image file named \"swastika.png\". The modality detected is an image. I need to analyze this image to determine if it contains hate symbols or hateful content.\n",
            "\n",
            "Action: ImageHateChecker\n",
            "Action Input: swastika.png\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-image-pipeline.hf.space ✔\n",
            "\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mImage Analysis Result: {'prediction': 'not_hate', 'language': None, 'label': 'No hate found, yay!', 'confidence': None, 'hate_text': None}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The ImageHateChecker tool has analyzed the image \"swastika.png\" and returned a result indicating that it does not contain hate content, with a label stating \"No hate found, yay!\". However, the swastika is a widely recognized hate symbol associated with Nazi ideology and white supremacy. I need to consider the context and the tool's confidence in its prediction.\n",
            "\n",
            "Action: None (since the tool has already been invoked)\n",
            "\n",
            "Thought: Although the ImageHateChecker tool returned a result indicating no hate content, I must consider the fact that a swastika is generally considered a hate symbol. The tool's output seems counterintuitive given the known significance of the swastika. I will have to rely on my own reasoning to make a final decision.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: Yes — Reason: The input image \"swastika.png\" contains a swastika, which is a widely recognized hate symbol associated with Nazi ideology and white supremacy. Despite the ImageHateChecker tool indicating \"No hate found\", the presence of a swastika is generally considered to be hateful or discriminatory content. Therefore, based on the known significance of the symbol, the input is classified as hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': '\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: image\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \"Hate Speech: Yes\" or \"Hate Speech: No\"\\n   - Followed by \"— Reason:\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\"Hate Speech: Yes/No — Reason: <your detailed explanation with all tool outputs and reasoning>\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: swastika.png\\n\\n\\nInput: swastika.png', 'output': 'Hate Speech: Yes — Reason: The input image \"swastika.png\" contains a swastika, which is a widely recognized hate symbol associated with Nazi ideology and white supremacy. Despite the ImageHateChecker tool indicating \"No hate found\", the presence of a swastika is generally considered to be hateful or discriminatory content. Therefore, based on the known significance of the symbol, the input is classified as hate speech.'}\n"
          ]
        }
      ],
      "source": [
        "# print(pipeline(\"chutiyo logo ko mumbai me lena hi nhi chahiye\"))\n",
        "print(pipeline(\"swastika.png\"))\n",
        "# print(pipeline(\"hate_audio.wav\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YI0E7lYIjY3L"
      },
      "outputs": [],
      "source": [
        "@traceable\n",
        "def evaluate_pipeline(test_dataset: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    results = []\n",
        "    for example in test_dataset:\n",
        "        inp = example.get(\"input\")\n",
        "        expected = example.get(\"expected\", \"N/A\")\n",
        "        output = pipeline(inp)\n",
        "        result = {\n",
        "            \"input\": inp,\n",
        "            \"expected\": expected,\n",
        "            \"output\": output,\n",
        "            \"modality\": detect_modality(inp)\n",
        "        }\n",
        "        results.append(result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "It8jSeQljY3L"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def extended_evaluate_pipeline(test_dataset: List[Dict[str, Any]], compute_metrics: bool = True):\n",
        "    results = []\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for example in test_dataset:\n",
        "        inp = example.get(\"input\")\n",
        "        expected = example.get(\"expected\")  # e.g., \"Yes\" or \"No\"\n",
        "\n",
        "        process = psutil.Process(os.getpid())\n",
        "        mem_before = process.memory_info().rss\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "        output = pipeline(inp)\n",
        "        end_time = time.perf_counter()\n",
        "        processing_time = end_time - start_time\n",
        "\n",
        "        mem_after = process.memory_info().rss\n",
        "        mem_used = mem_after - mem_before\n",
        "\n",
        "        # Assume output follows: \"Hate Speech: Yes — Reason: <...>\"\n",
        "        predicted = \"Unknown\"\n",
        "        if \"Hate Speech:\" in output:\n",
        "            try:\n",
        "                predicted = output.split(\"Hate Speech:\")[1].split(\"—\")[0].strip()\n",
        "            except Exception:\n",
        "                predicted = output.split(\"Hate Speech:\")[1].strip()\n",
        "\n",
        "        result = {\n",
        "            \"input\": inp,\n",
        "            \"expected\": expected,\n",
        "            \"output\": output,\n",
        "            \"predicted\": predicted,\n",
        "            \"processing_time\": processing_time,\n",
        "            \"memory_used_bytes\": mem_used,\n",
        "            \"modality\": detect_modality(inp)\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        if expected is not None:\n",
        "            ground_truths.append(expected)\n",
        "            predictions.append(predicted)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oYR8bKTQjY3L"
      },
      "outputs": [],
      "source": [
        "test_dataset = [\n",
        "    {\"input\": \"chutiyo logo ko mumbai me lena hi nhi chahiye\", \"expected\": \"Yes\"},\n",
        "    {\"input\": \"I love the diverse culture of this city\", \"expected\": \"No\"},\n",
        "    {\"input\": \"swastika.png\", \"expected\": \"Yes\"},\n",
        "    {\"input\": \"hate_audio.wav\", \"expected\": \"Yes\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKKzoJjKjY3L",
        "outputId": "175bb6c9-a3cd-492a-f790-49c60bdfdd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe input is given in Hinglish (a mix of Hindi and English). The first step is to analyze this text for hate speech using the appropriate tool.\n",
            "\n",
            "Action: HinglishHateChecker\n",
            "Action Input: chutiyo logo ko mumbai me lena hi nhi chahiye\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-hinglish-text-model.hf.space ✔\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mPrediction: OAG with a score of 0.9616120457649231\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe HinglishHateChecker tool has detected hate speech in the given text with a high score, indicating a strong likelihood that the text contains hate speech. To further understand the context and sentiment, I will analyze the sentiment of the text.\n",
            "\n",
            "Action: SentimentAnalyzer\n",
            "Action Input: chutiyo logo ko mumbai me lena hi nhi chahiye\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m[[{'label': 'POSITIVE', 'score': 0.6149528622627258}, {'label': 'NEGATIVE', 'score': 0.3850470781326294}]]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe sentiment analysis indicates a slightly positive sentiment, which seems counterintuitive given the context. However, the primary indication from the HinglishHateChecker suggests that the text is likely to be hate speech. I will now consolidate these findings to make a final decision.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: Yes — Reason: The HinglishHateChecker tool detected hate speech with a high score of 0.9616120457649231. Although the SentimentAnalyzer indicated a slightly positive sentiment (score: 0.6149528622627258), the presence of a derogatory term ('chutiyo') and the context suggest that the text is derogatory and discriminatory, aligning with the definition of hate speech. The sentiment analysis result may be misleading due to the complexity of detecting sentiment in Hinglish text or the specific wording used. Therefore, based on the strong indication from the HinglishHateChecker and understanding the derogatory nature of the text, it is classified as hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe input is a text, so I need to determine if it's in English or Hinglish and then use the appropriate tool to detect hate speech. The text \"I love the diverse culture of this city\" appears to be in English.\n",
            "\n",
            "Action: EnglishHateChecker\n",
            "Action Input: \"I love the diverse culture of this city\"\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-english-text-model.hf.space ✔\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mPrediction: NEITHER with a score of 0.9985595345497131\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe EnglishHateChecker tool has classified the input as \"NEITHER\" with a high score, indicating that it does not contain hate speech. To further validate this, I can analyze the sentiment of the text.\n",
            "\n",
            "Action: SentimentAnalyzer\n",
            "Action Input: \"I love the diverse culture of this city\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m[[{'label': 'POSITIVE', 'score': 0.9998651742935181}, {'label': 'NEGATIVE', 'score': 0.00013480843335855752}]]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe SentimentAnalyzer tool has classified the sentiment of the input as \"POSITIVE\" with a very high score, indicating a strong positive emotional tone. This aligns with the output from the EnglishHateChecker, which did not detect hate speech. Given that both tools indicate the absence of hate speech and the presence of a positive sentiment, I can conclude that the input does not constitute hate speech.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: No — Reason: The input \"I love the diverse culture of this city\" is classified as NEITHER by the EnglishHateChecker with a high score, indicating no hate speech. The SentimentAnalyzer further supports this by classifying the sentiment as POSITIVE with a very high score, showing a strong positive emotional tone. Both analyses confirm that the input does not contain hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The input is an image file named \"swastika.png\". The modality detected is an image. I need to analyze this image to determine if it contains hate symbols or hateful content.\n",
            "\n",
            "Action: ImageHateChecker\n",
            "Action Input: swastika.png\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-image-pipeline.hf.space ✔\n",
            "\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mImage Analysis Result: {'prediction': 'not_hate', 'language': None, 'label': 'No hate found, yay!', 'confidence': None, 'hate_text': None}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The ImageHateChecker tool has analyzed the image \"swastika.png\" and returned a result indicating that it does not contain hate content, with a label stating \"No hate found, yay!\". However, the swastika is a well-known hate symbol associated with Nazi ideology and white supremacy. I need to consider the context and the tool's confidence in its prediction.\n",
            "\n",
            "Action: None (since the tool has already been invoked)\n",
            "\n",
            "Thought: Although the ImageHateChecker tool returned a result indicating no hate content, I must consider the fact that a swastika is generally regarded as a hate symbol. The tool's output seems counterintuitive given the known significance of the swastika. I will have to rely on my own reasoning to make a final decision.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: Yes — Reason: The input image \"swastika.png\" contains a swastika, which is a widely recognized hate symbol associated with Nazi ideology and white supremacy. Despite the ImageHateChecker tool indicating \"No hate found\", the presence of a swastika is generally considered to be hateful or discriminatory content. Therefore, based on the known significance of the swastika, I conclude that the image constitutes hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The input is an audio file named \"hate_audio.wav\". The modality detected is audio. According to the instructions, for audio files, I should use the AudioHateChecker tool.\n",
            "\n",
            "Action: AudioHateChecker\n",
            "Action Input: hate_audio.wav\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-audio-pipeline.hf.space ✔\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mAudio Analysis Result: {'prediction': 'hate', 'language': 'Hinglish', 'label': None, 'confidence': 0.3557116587956746, 'hate_text': 'that they have flash type of shows and turf war or gosh or ah yes ah ah the can share say that where i want one that i were you wish to how the flu or shop on oh oh oh oh oh oh oh'}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The AudioHateChecker tool has provided an analysis result for the audio file \"hate_audio.wav\". The prediction is 'hate', indicating that the audio content is likely to be hate speech. However, the confidence level is 0.3557116587956746, which is relatively low. The language detected is Hinglish, and there's a 'hate_text' field that seems to contain a transcription of the audio content, although it appears to be somewhat garbled or nonsensical.\n",
            "\n",
            "To further assess the situation, I should analyze the sentiment of the transcribed text to understand its emotional tone.\n",
            "\n",
            "Action: SentimentAnalyzer\n",
            "Action Input: \"that they have flash type of shows and turf war or gosh or ah yes ah ah the can share say that where i want one that i were you wish to how the flu or shop on oh oh oh oh oh oh oh\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m[[{'label': 'NEGATIVE', 'score': 0.9914012551307678}, {'label': 'POSITIVE', 'score': 0.008598752319812775}]]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The SentimentAnalyzer tool has analyzed the sentiment of the transcribed text from the audio file. The result indicates a strong negative sentiment with a score of 0.9914012551307678, suggesting that the emotional tone of the text is predominantly negative.\n",
            "\n",
            "Given the AudioHateChecker's prediction of 'hate' and the SentimentAnalyzer's indication of a strong negative sentiment, I should now consolidate these findings to make a final decision on whether the input audio constitutes hate speech.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: Yes — Reason: The AudioHateChecker tool predicted the audio content as 'hate' with a confidence level of 0.3557116587956746, and the SentimentAnalyzer indicated a strong negative sentiment (score: 0.9914012551307678) in the transcribed text. Although the confidence level from the AudioHateChecker is relatively low, the strong negative sentiment supports the conclusion that the audio content is hate speech. Therefore, based on the consolidated outputs and reasoning, the input audio is classified as hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "eval_results = evaluate_pipeline(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGnHTmNljY3M",
        "outputId": "28b74931-12b2-4b0b-878d-a36e30042149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "{\n",
            "  \"input\": \"chutiyo logo ko mumbai me lena hi nhi chahiye\",\n",
            "  \"expected\": \"Yes\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: text\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: chutiyo logo ko mumbai me lena hi nhi chahiye\\n\\n\\nInput: chutiyo logo ko mumbai me lena hi nhi chahiye\",\n",
            "    \"output\": \"Hate Speech: Yes \\u2014 Reason: The HinglishHateChecker tool detected hate speech with a high score of 0.9616120457649231. Although the SentimentAnalyzer indicated a slightly positive sentiment (score: 0.6149528622627258), the presence of a derogatory term ('chutiyo') and the context suggest that the text is derogatory and discriminatory, aligning with the definition of hate speech. The sentiment analysis result may be misleading due to the complexity of detecting sentiment in Hinglish text or the specific wording used. Therefore, based on the strong indication from the HinglishHateChecker and understanding the derogatory nature of the text, it is classified as hate speech.\"\n",
            "  },\n",
            "  \"modality\": \"text\"\n",
            "}\n",
            "{\n",
            "  \"input\": \"I love the diverse culture of this city\",\n",
            "  \"expected\": \"No\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: text\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: I love the diverse culture of this city\\n\\n\\nInput: I love the diverse culture of this city\",\n",
            "    \"output\": \"Hate Speech: No \\u2014 Reason: The input \\\"I love the diverse culture of this city\\\" is classified as NEITHER by the EnglishHateChecker with a high score, indicating no hate speech. The SentimentAnalyzer further supports this by classifying the sentiment as POSITIVE with a very high score, showing a strong positive emotional tone. Both analyses confirm that the input does not contain hate speech.\"\n",
            "  },\n",
            "  \"modality\": \"text\"\n",
            "}\n",
            "{\n",
            "  \"input\": \"swastika.png\",\n",
            "  \"expected\": \"Yes\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: image\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: swastika.png\\n\\n\\nInput: swastika.png\",\n",
            "    \"output\": \"Hate Speech: Yes \\u2014 Reason: The input image \\\"swastika.png\\\" contains a swastika, which is a widely recognized hate symbol associated with Nazi ideology and white supremacy. Despite the ImageHateChecker tool indicating \\\"No hate found\\\", the presence of a swastika is generally considered to be hateful or discriminatory content. Therefore, based on the known significance of the swastika, I conclude that the image constitutes hate speech.\"\n",
            "  },\n",
            "  \"modality\": \"image\"\n",
            "}\n",
            "{\n",
            "  \"input\": \"hate_audio.wav\",\n",
            "  \"expected\": \"Yes\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: audio\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: hate_audio.wav\\n\\n\\nInput: hate_audio.wav\",\n",
            "    \"output\": \"Hate Speech: Yes \\u2014 Reason: The AudioHateChecker tool predicted the audio content as 'hate' with a confidence level of 0.3557116587956746, and the SentimentAnalyzer indicated a strong negative sentiment (score: 0.9914012551307678) in the transcribed text. Although the confidence level from the AudioHateChecker is relatively low, the strong negative sentiment supports the conclusion that the audio content is hate speech. Therefore, based on the consolidated outputs and reasoning, the input audio is classified as hate speech.\"\n",
            "  },\n",
            "  \"modality\": \"audio\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluation Results:\")\n",
        "for res in eval_results:\n",
        "    print(json.dumps(res, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQMztApVjY3M",
        "outputId": "e8139ebb-9f81-4f86-d002-1c0e8de280e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe input is text and appears to be in Hinglish. The first step is to analyze this text for hate speech using the appropriate tool.\n",
            "\n",
            "Action: HinglishHateChecker\n",
            "Action Input: chutiyo logo ko mumbai me lena hi nhi chahiye\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-hinglish-text-model.hf.space ✔\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mPrediction: OAG with a score of 0.9616120457649231\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe HinglishHateChecker tool has returned a prediction of \"OAG\" with a high score, indicating that the text is likely to be hate speech or offensive. However, to be thorough, I should also analyze the sentiment of the text to understand its emotional tone.\n",
            "\n",
            "Action: SentimentAnalyzer\n",
            "Action Input: chutiyo logo ko mumbai me lena hi nhi chahiye\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m[[{'label': 'POSITIVE', 'score': 0.6149528622627258}, {'label': 'NEGATIVE', 'score': 0.3850470781326294}]]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe SentimentAnalyzer tool has returned a mixed sentiment, but leaning towards positive, which seems counterintuitive given the context. However, the primary indication from the HinglishHateChecker suggests the text is offensive or hate speech. I will now make a final decision based on these observations.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: Yes — Reason: The HinglishHateChecker tool classified the input text as \"OAG\" with a high score of 0.9616120457649231, indicating it is likely hate speech or offensive. Although the SentimentAnalyzer provided a mixed sentiment leaning towards positive, which is not aligned with the expected negative sentiment for hate speech, the strong indication from the HinglishHateChecker suggests that the text is hate speech. The text contains derogatory language and expresses a negative attitude towards a group of people, supporting the conclusion that it is hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe input is a text, so I need to determine if it's in English or Hinglish and then analyze it for hate speech and sentiment.\n",
            "\n",
            "Action: EnglishHateChecker\n",
            "Action Input: I love the diverse culture of this city\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-english-text-model.hf.space ✔\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mPrediction: NEITHER with a score of 0.9985595345497131\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe EnglishHateChecker has classified the input as \"NEITHER\" with a high score, indicating it does not contain hate speech. Now, I will analyze the sentiment of the text to further understand its emotional tone.\n",
            "\n",
            "Action: SentimentAnalyzer\n",
            "Action Input: I love the diverse culture of this city\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m[[{'label': 'POSITIVE', 'score': 0.9998651742935181}, {'label': 'NEGATIVE', 'score': 0.00013480843335855752}]]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe SentimentAnalyzer has classified the sentiment of the input as \"POSITIVE\" with a very high score, indicating a strongly positive emotional tone. Given that the EnglishHateChecker did not detect hate speech and the sentiment is positive, I can conclude that the input is not hate speech.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: No — Reason: The input \"I love the diverse culture of this city\" was analyzed using the EnglishHateChecker, which classified it as \"NEITHER\" with a score of 0.9985595345497131, indicating no hate speech. Further, the SentimentAnalyzer showed a strongly positive sentiment with a score of 0.9998651742935181. Both analyses confirm that the input is not hate speech and has a positive emotional tone.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The input is an image file named \"swastika.png\". The modality detected is an image. I need to analyze this image to determine if it contains hate symbols or hateful content.\n",
            "\n",
            "Action: ImageHateChecker\n",
            "Action Input: swastika.png\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-image-pipeline.hf.space ✔\n",
            "\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mImage Analysis Result: {'prediction': 'not_hate', 'language': None, 'label': 'No hate found, yay!', 'confidence': None, 'hate_text': None}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The ImageHateChecker tool has analyzed the image \"swastika.png\" and returned a result indicating that it does not contain hate content, with the label 'No hate found, yay!'. However, the swastika is a well-known hate symbol associated with Nazi ideology and white supremacy. I need to consider the context and the tool's confidence in its prediction.\n",
            "\n",
            "Action: None (since the tool has already been invoked)\n",
            "\n",
            "Thought: Although the ImageHateChecker returned a 'not_hate' prediction, the swastika symbol is widely recognized as a hate symbol. The tool's output seems counterintuitive given the known connotations of the swastika. I need to use my reasoning to decide whether the input constitutes hate speech.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: Yes — Reason: The image \"swastika.png\" contains a swastika, which is a widely recognized hate symbol associated with Nazi ideology and white supremacy. Despite the ImageHateChecker tool's output indicating 'not_hate', the presence of this symbol is generally considered to be hateful or associated with hate groups. Therefore, it is reasonable to classify this image as hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The input is an audio file named \"hate_audio.wav\". The modality detected is audio. According to the instructions, for audio files, I should use the AudioHateChecker tool.\n",
            "\n",
            "Action: AudioHateChecker\n",
            "Action Input: hate_audio.wav\u001b[0mLoaded as API: https://dj-dawgs-ipd-ipd-audio-pipeline.hf.space ✔\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mAudio Analysis Result: {'prediction': 'not_hate', 'language': None, 'label': None, 'confidence': None, 'hate_text': None}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The AudioHateChecker tool has returned a result indicating that the audio content is 'not_hate'. However, the output lacks specific details such as language, label, confidence, and hate_text. I will have to rely on this result to make my final decision.\n",
            "\n",
            "Action: None\n",
            "Action Input: None\u001b[0m\n",
            "Observation: None is not a valid tool, try one of [HinglishHateChecker, EnglishHateChecker, ImageHateChecker, AudioHateChecker, SentimentAnalyzer].\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I have already used the appropriate tool for the audio input, which is the AudioHateChecker. The result indicates that the audio content is 'not_hate'. Although the output lacks detailed information, I will proceed with this result to make my final decision.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Hate Speech: No — Reason: The AudioHateChecker tool analyzed the audio file \"hate_audio.wav\" and predicted that it does not contain hate speech. The prediction result was 'not_hate'. Although the output lacked additional details such as language, label, confidence, and specific hate text, the primary indication is that the audio content is not hate speech.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 'Yes') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 'Yes') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1618: UserWarning: Note that pos_label (set to 'Yes') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "eval_results = extended_evaluate_pipeline(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_FQu0sSjY3M",
        "outputId": "66b3f90b-ea96-4b0d-f2f4-f4266bfe475f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "{\n",
            "  \"input\": \"chutiyo logo ko mumbai me lena hi nhi chahiye\",\n",
            "  \"expected\": \"Yes\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: text\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: chutiyo logo ko mumbai me lena hi nhi chahiye\\n\\n\\nInput: chutiyo logo ko mumbai me lena hi nhi chahiye\",\n",
            "    \"output\": \"Hate Speech: Yes \\u2014 Reason: The HinglishHateChecker tool classified the input text as \\\"OAG\\\" with a high score of 0.9616120457649231, indicating it is likely hate speech or offensive. Although the SentimentAnalyzer provided a mixed sentiment leaning towards positive, which is not aligned with the expected negative sentiment for hate speech, the strong indication from the HinglishHateChecker suggests that the text is hate speech. The text contains derogatory language and expresses a negative attitude towards a group of people, supporting the conclusion that it is hate speech.\"\n",
            "  },\n",
            "  \"predicted\": \"Unknown\",\n",
            "  \"processing_time\": 9.964922613999988,\n",
            "  \"memory_used_bytes\": 2129920,\n",
            "  \"modality\": \"text\"\n",
            "}\n",
            "{\n",
            "  \"input\": \"I love the diverse culture of this city\",\n",
            "  \"expected\": \"No\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: text\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: I love the diverse culture of this city\\n\\n\\nInput: I love the diverse culture of this city\",\n",
            "    \"output\": \"Hate Speech: No \\u2014 Reason: The input \\\"I love the diverse culture of this city\\\" was analyzed using the EnglishHateChecker, which classified it as \\\"NEITHER\\\" with a score of 0.9985595345497131, indicating no hate speech. Further, the SentimentAnalyzer showed a strongly positive sentiment with a score of 0.9998651742935181. Both analyses confirm that the input is not hate speech and has a positive emotional tone.\"\n",
            "  },\n",
            "  \"predicted\": \"Unknown\",\n",
            "  \"processing_time\": 9.827456274000042,\n",
            "  \"memory_used_bytes\": 1200128,\n",
            "  \"modality\": \"text\"\n",
            "}\n",
            "{\n",
            "  \"input\": \"swastika.png\",\n",
            "  \"expected\": \"Yes\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: image\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: swastika.png\\n\\n\\nInput: swastika.png\",\n",
            "    \"output\": \"Hate Speech: Yes \\u2014 Reason: The image \\\"swastika.png\\\" contains a swastika, which is a widely recognized hate symbol associated with Nazi ideology and white supremacy. Despite the ImageHateChecker tool's output indicating 'not_hate', the presence of this symbol is generally considered to be hateful or associated with hate groups. Therefore, it is reasonable to classify this image as hate speech.\"\n",
            "  },\n",
            "  \"predicted\": \"Unknown\",\n",
            "  \"processing_time\": 12.457918823,\n",
            "  \"memory_used_bytes\": 806912,\n",
            "  \"modality\": \"image\"\n",
            "}\n",
            "{\n",
            "  \"input\": \"hate_audio.wav\",\n",
            "  \"expected\": \"Yes\",\n",
            "  \"output\": {\n",
            "    \"input\": \"\\nYou are an intelligent AI agent specialized in detecting hate speech. Your objective is to analyze the given input and decide if it constitutes hate speech.\\n\\nModality Detected: audio\\n\\nUse the following tools as applicable:\\n- For text inputs (both English and Hinglish): use the EnglishHateChecker and HinglishHateChecker to detect hate content, and use the SentimentAnalyzer to analyze sentiment.\\n- For image files: use the ImageHateChecker.\\n- For audio files: use the AudioHateChecker.\\n\\nInstructions:\\n1. **Input Analysis:** First, determine the modality of the input. If it is text, detect the language (English or Hinglish). Document your reasoning as a chain-of-thought.\\n2. **Tool Invocation:** Based on your analysis, invoke the appropriate tool(s). DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\n4. **Final Decision:** Using the consolidated outputs and your step-by-step reasoning, decide if the input is hate speech. Your final output must state:\\n   - \\\"Hate Speech: Yes\\\" or \\\"Hate Speech: No\\\"\\n   - Followed by \\\"\\u2014 Reason:\\\" and a comprehensive explanation that integrates all tool outputs and your reasoning.\\n\\nFinal Output Format:\\n\\\"Hate Speech: Yes/No \\u2014 Reason: <your detailed explanation with all tool outputs and reasoning>\\\"\\n\\nIMPORTANT: DO NOT REPEAT USING THE TOOLS IF THEY ARE NOT WORKING. SKIP OVER THEM AND USE YOUR OWN REASONING. \\nSOMETIMES THE TOOL MAY GIVE WRONG OUTPUT, SO TAKE INTO CONSIDERATION YOUR OWN REASONING AND DO NOT JUSTIFY THE OUTPUT BASED ON THE TOOL OUTPUT.\\n\\nNow, process the following input:\\nInput: hate_audio.wav\\n\\n\\nInput: hate_audio.wav\",\n",
            "    \"output\": \"Hate Speech: No \\u2014 Reason: The AudioHateChecker tool analyzed the audio file \\\"hate_audio.wav\\\" and predicted that it does not contain hate speech. The prediction result was 'not_hate'. Although the output lacked additional details such as language, label, confidence, and specific hate text, the primary indication is that the audio content is not hate speech.\"\n",
            "  },\n",
            "  \"predicted\": \"Unknown\",\n",
            "  \"processing_time\": 97.73174307,\n",
            "  \"memory_used_bytes\": 1617920,\n",
            "  \"modality\": \"audio\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluation Results:\")\n",
        "for res in eval_results:\n",
        "    print(json.dumps(res, indent=2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}